{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-13T13:48:01.363486Z",
     "start_time": "2024-12-13T13:48:01.360941Z"
    }
   },
   "source": [
    "import onnx\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "from mlvot.tp1.KalmanFilter import KalmanFilter\n",
    "from mlvot.tp2_3.tp2 import get_sim_matrix, get_assignments, update_tracks_with_ids\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T13:34:22.503810Z",
     "start_time": "2024-12-13T13:34:22.490030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FeatureExtractor:\n",
    "    def __init__(self, model_path=\"reid_osnet_x025_market1501.onnx\", input_size=(64, 128)):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = onnx.load(model_path)\n",
    "        self.model.eval()\n",
    "        self.input_size = input_size\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(self.input_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "    def preprocess_patch(self, patch):\n",
    "        patch = cv2.cvtColor(patch, cv2.COLOR_BGR2RGB)\n",
    "        return self.transform(patch).unsqueeze(0).to(self.device)\n",
    "\n",
    "    def extract_features(self, im_crops):\n",
    "        features = []\n",
    "        for patch in im_crops:\n",
    "            preprocessed_patch = self.preprocess_patch(patch)\n",
    "            with torch.no_grad():\n",
    "                feature = self.model(preprocessed_patch)\n",
    "            features.append(feature.cpu().numpy())\n",
    "        return np.vstack(features)\n"
   ],
   "id": "9f9e01625802ce55",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T13:37:51.871172Z",
     "start_time": "2024-12-13T13:37:51.869102Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_similarity(features1, features2, metric=\"cosine\"):\n",
    "    return 1 - cdist(features1, features2, metric)\n"
   ],
   "id": "989d02a3bdfcdced",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T13:37:52.391081Z",
     "start_time": "2024-12-13T13:37:52.388387Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_combined_sim_matrix(bbox1, features1, bbox2, features2, alpha=0.5, beta=0.5):\n",
    "    iou_matrix = get_sim_matrix(bbox1, bbox2)\n",
    "    feature_similarity_matrix = compute_similarity(features1, features2, metric=\"cosine\")\n",
    "\n",
    "    feature_similarity_matrix = (feature_similarity_matrix - feature_similarity_matrix.min()) / (\n",
    "        feature_similarity_matrix.max() - feature_similarity_matrix.min()\n",
    "    )\n",
    "\n",
    "    return alpha * iou_matrix + beta * feature_similarity_matrix\n"
   ],
   "id": "92fcff784f5ecc68",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def track_objects(video_path, feature_extractor):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    tracked_bboxs = []\n",
    "    tracked_features = []\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Simulate detection step (Replace with actual detector)\n",
    "        detections = []  # List of bounding boxes from the detector\n",
    "        im_crops = []    # Extract image patches for detected bounding boxes\n",
    "\n",
    "        # Extract features for detections\n",
    "        detected_features = feature_extractor.extract_features(im_crops)\n",
    "\n",
    "        # Calculate similarity matrix\n",
    "        sim_matrix = get_combined_sim_matrix(tracked_bboxs, tracked_features, detections, detected_features)\n",
    "\n",
    "        # Assign detections to tracked objects\n",
    "        assignments = get_assignments(sim_matrix)  # Implement your assignment function\n",
    "\n",
    "        # Update tracks\n",
    "        update_tracks_with_ids(tracked_bboxs, detections, assignments)  # Implement your update logic\n",
    "\n",
    "        # Update features for tracked objects\n",
    "        tracked_features = [detected_features[i] for i in assignments]\n",
    "\n",
    "        # Visualize or process frame (optional)\n",
    "        # ...\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "\n",
    "# Run the tracker\n",
    "if __name__ == \"__main__\":\n",
    "    feature_extractor = FeatureExtractor()\n",
    "    video_path = \"randomball.avi\"\n",
    "    track_objects(video_path, feature_extractor)"
   ],
   "id": "e704fe55d2bc8b0"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-12-13T13:48:19.482069Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from mlvot.tp2_3.tp2 import save_video\n",
    "from mlvot.tp1.Detector import detect\n",
    "# Import libraries\n",
    "import cv2\n",
    "from time import sleep\n",
    "\n",
    "# Main object tracking loop\n",
    "def main(video_path, output_video_path):\n",
    "    # Initialize Kalman Filter\n",
    "    kf = KalmanFilter(0.1, 1, 1, 1, 0.1, 0.1)\n",
    "    trajectory = []  # To store trajectory points\n",
    "    frames = []  # To store video frames with tracking overlays\n",
    "\n",
    "    # Open video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video file.\")\n",
    "        return\n",
    "\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "    # Process video frames\n",
    "    while True:\n",
    "        sleep(0.1)  # Slow down for visualization purposes\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"End of video stream.\")\n",
    "            break\n",
    "\n",
    "        # Detect objects in the frame\n",
    "        centers = detect(frame)\n",
    "\n",
    "        # Track objects using Kalman Filter\n",
    "        for center in centers:\n",
    "            # Kalman Filter Prediction\n",
    "            kf.predict()\n",
    "            predicted_x, predicted_y = map(int, kf.x_k[:2].flatten())\n",
    "            cv2.rectangle(frame, (predicted_x - 10, predicted_y - 10),\n",
    "                          (predicted_x + 10, predicted_y + 10), (255, 0, 0), 2)\n",
    "\n",
    "            # Kalman Filter Update with detected center\n",
    "            kf.update(center)\n",
    "            estimated_x, estimated_y = map(int, kf.x_k[:2].flatten())\n",
    "            cv2.rectangle(frame, (estimated_x - 10, estimated_y - 10),\n",
    "                          (estimated_x + 10, estimated_y + 10), (0, 0, 255), 2)\n",
    "\n",
    "            # Draw trajectory\n",
    "            trajectory.append((estimated_x, estimated_y))\n",
    "            for point in trajectory:\n",
    "                cv2.circle(frame, point, 1, (0, 0, 0), -1)\n",
    "\n",
    "            # Draw detected center\n",
    "            cv2.circle(frame, (int(center[0]), int(center[1])), 5, (0, 255, 0), -1)\n",
    "\n",
    "        # Append the processed frame for saving\n",
    "        frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "        # Display the frame\n",
    "        cv2.imshow('Object Tracking', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            print(\"Terminating video processing...\")\n",
    "            break\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Save the output video\n",
    "    save_video(frames, output_video_path, fps=fps, frame_size=(frame_width, frame_height))\n",
    "    print(f\"Processed video saved to {output_video_path}\")\n",
    "\n",
    "# Run the script\n",
    "if __name__ == \"__main__\":\n",
    "    video_input_path = \"../tp1/randomball.avi\"\n",
    "    video_output_path = \"tracked_output.avi\"\n",
    "    main(video_input_path, video_output_path)\n"
   ],
   "id": "38d2539f6ffdc9d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c8fba8a25646f138"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
